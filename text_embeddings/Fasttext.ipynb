{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fasttext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F9BoxhU3sJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os \n",
        "os.chdir('/content/drive/My Drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ywyDhzp4Ewe",
        "colab_type": "code",
        "outputId": "51c2fd30-dfaf-433f-b58a-92d5c0a49092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (45.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2386053 sha256=a1c994b076ccc9e6255eda9ffafe7fe84ce2bac9a7b75c8ccf23a8e7925ec59e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVt-DF_d30Pz",
        "colab_type": "code",
        "outputId": "1b4e873e-79fd-40b1-b7c9-c32f6897229a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "import fasttext\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Read keywords\n",
        "keywords_list = open(\"train_keywords.pkl\", \"rb\")\n",
        "keywords_list = pickle.load(keywords_list)\n",
        "\n",
        "# Read training data\n",
        "with open(\"train.csv\", 'r') as f:\n",
        "    train_data = f.read().splitlines()\n",
        "\n",
        "train_hosts = list()\n",
        "y_train = list()\n",
        "for row in train_data:\n",
        "    host, label = row.split(\",\")\n",
        "    train_hosts.append(host)\n",
        "    y_train.append(label.lower())\n",
        "\n",
        "# Load fast text\n",
        "ft = fasttext.load_model('cc.fr.300.bin')\n",
        "\n",
        "# Get Embeddings\n",
        "filenames = train_hosts\n",
        "doc_embeddings = dict.fromkeys(train_hosts)\n",
        "for i,file in enumerate(filenames):\n",
        "    keywords = keywords_list[i]\n",
        "    if len(keywords)<1:\n",
        "        continue\n",
        "    doc_embedding = np.zeros(300)\n",
        "    for token in keywords:\n",
        "        doc_embedding += ft.get_word_vector(token)\n",
        "    doc_embedding /= len(keywords)\n",
        "    doc_embeddings[file] = doc_embedding\n",
        "    if i%100==0:\n",
        "        print(i)\n",
        "\n",
        "# Save Embeddings\n",
        "with open('doc_keywords_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(doc_embeddings, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J-S6NRm4PBp",
        "colab_type": "code",
        "outputId": "f0c85b79-c453-43f8-d92c-a1316850f31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Read keywords\n",
        "keywords_list = open(\"test_keywords.pkl\", \"rb\")\n",
        "keywords_list = pickle.load(keywords_list)\n",
        "\n",
        "# Read test data\n",
        "with open(\"test.csv\", 'r') as f:\n",
        "    test_data = f.read().splitlines()\n",
        "    \n",
        "with open(\"test.csv\", 'r') as f:\n",
        "    test_hosts = f.read().splitlines()\n",
        "\n",
        "# Load fast text\n",
        "# ft = fasttext.load_model('cc.fr.300.bin')\n",
        "\n",
        "# Get Embeddings\n",
        "filenames = test_hosts\n",
        "doc_embeddings = dict.fromkeys(test_hosts)\n",
        "for i,file in enumerate(filenames):\n",
        "    keywords = keywords_list[i]\n",
        "    if len(keywords)<1:\n",
        "        continue\n",
        "    doc_embedding = np.zeros(300)\n",
        "    for token in keywords:\n",
        "        doc_embedding += ft.get_word_vector(token)\n",
        "    doc_embedding /= len(keywords)\n",
        "    doc_embeddings[file] = doc_embedding\n",
        "    if i%100==0:\n",
        "        print(i)\n",
        "\n",
        "# Save Embeddings\n",
        "with open('test_keywords_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(doc_embeddings, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}